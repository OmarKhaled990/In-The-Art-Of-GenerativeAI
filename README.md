# In-The-Art-Of-GenerativeAI

The goal of my project was to develop a system that could generate visually appealing images based on textual descriptions. To achieve this, I implemented the Stable Diffusion method, which combines the power of diffusion models with text embeddings. This unique approach allows for the creation of high-quality and imaginative images guided by textual prompts.


Throughout the project, I successfully integrated Variational Autoencoders (VAEs) and diffusion models into the pipeline. The VAEs were responsible for encoding and decoding images, while the diffusion models refined the latent representations using noise predictions. This combination ensured that the generated images captured the essence of the textual descriptions in a visually stunning manner.


To make the system text-guided, I incorporated CLIP Text Embeddings. These embeddings helped bridge the gap between text and images by guiding the diffusion models to generate images aligned with the provided prompts. The result was a seamless fusion of textual context and visual creativity.


I generated some Images from the pipeline and let's say they are magnificent, you can even compare the generated image with real ones without seeing any difference.
